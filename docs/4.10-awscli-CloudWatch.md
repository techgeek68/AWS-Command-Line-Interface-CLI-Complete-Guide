# AWS CLI — CloudWatch & Logs Operations Guide

---
> Replace sample IDs (InstanceId, FunctionName, ARNs, log groups) with your actual values.
---
> Focus areas: Metrics, Custom Metrics, Metric Math, Alarms (standard, composite, anomaly), Logs (groups, streams, ingestion, Insights), Dashboards, Anomaly Detection, Export, Subscriptions, Troubleshooting, and Cleanup.
---

## Table of Contents
1. Overview  
2. Quick Reference Variables  
3. Metrics (List & Get)  
4. Custom Metrics & Dimensions  
5. Metric Math & Anomaly Detection  
6. Alarms (Standard, Composite, Anomaly)  
7. Alarm Management & Actions  
8. Logs: Groups, Streams, Retention  
9. Log Ingestion (Manual / Sequence Tokens)  
10. Tail & Filter Patterns  
11. CloudWatch Logs Insights Queries  
12. Subscription Filters (Lambda / Firehose / Kinesis)  
13. Export Log Data to S3  
14. Dashboards  
15. Metrics (Lambda & Cross-Namespace Tips)  
16. Best Practices  
17. Troubleshooting  
18. Automation Snippets  
19. Cleanup Sequences  
20. Notes  

---

## 1. Overview
Amazon CloudWatch provides metrics (time‑series), alarms, dashboards, events, logs, and analytics (Logs Insights). CloudWatch Logs aggregates application/system logs; Insights runs ad‑hoc queries. This guide uses UTC for time functions via `date -u`.

---

## 2. Quick Reference Variables
```bash
export REGION=us-east-1
export INSTANCE_ID=i-0a1b2c3d4e5f67890
export FUNCTION_NAME=hello-api
export NAMESPACE_CUSTOM="Custom/App"
export ALARM_CPU=ec2-cpu-high
export LOG_GROUP=/app/web/production
export LOG_STREAM=app-web-1
export QUEUE_ARN=arn:aws:sqs:us-east-1:123456789012:app-log-dlq
export SNS_TOPIC_ARN=arn:aws:sns:us-east-1:123456789012:ops-alerts
export S3_EXPORT_BUCKET=cloudwatch-log-exports-dev
```

---

## 3. Metrics (List & Get)

### 3.1 List namespaces
Syntax:
```bash
aws cloudwatch list-metrics --query 'Metrics[].Namespace' --output text | sort -u
```
Example:
```bash
aws cloudwatch list-metrics --query 'Metrics[].Namespace' --output text | tr '\t' '\n' | sort -u | head
```

### 3.2 List EC2 CPU metrics
Syntax:
```bash
aws cloudwatch list-metrics --namespace AWS/EC2 --metric-name CPUUtilization
```
Example (limit to instance):
```bash
aws cloudwatch list-metrics \
  --namespace AWS/EC2 \
  --metric-name CPUUtilization \
  --dimensions Name=InstanceId,Value=$INSTANCE_ID
```

### 3.3 Get metric statistics (recent 30m)
Syntax:
```bash
aws cloudwatch get-metric-statistics \
  --namespace <NAMESPACE> \
  --metric-name <METRIC> \
  --dimensions Name=<DIM_NAME>,Value=<DIM_VALUE> \
  --start-time <START_ISO> --end-time <END_ISO> \
  --period <SECONDS> \
  --statistics Average Maximum
```
Example:
```bash
aws cloudwatch get-metric-statistics \
  --namespace AWS/EC2 \
  --metric-name CPUUtilization \
  --dimensions Name=InstanceId,Value=$INSTANCE_ID \
  --start-time "$(date -u -d '30 minutes ago' +%FT%TZ)" \
  --end-time   "$(date -u +%FT%TZ)" \
  --period 60 \
  --statistics Average Maximum \
  --query 'Datapoints|sort_by(@,&Timestamp)[].{T:Timestamp,Avg:Average,Max:Maximum}'
```

### 3.4 Latest Lambda Invocations datapoint
Syntax:
```bash
aws cloudwatch get-metric-statistics --namespace AWS/Lambda --metric-name Invocations \
  --dimensions Name=FunctionName,Value=<FUNCTION_NAME> \
  --start-time <START_ISO> --end-time <END_ISO> --period 60 --statistics Sum \
  --query 'Datapoints|sort_by(@,&Timestamp)[-1]'
```
Example:
```bash
aws cloudwatch get-metric-statistics \
  --namespace AWS/Lambda \
  --metric-name Invocations \
  --dimensions Name=FunctionName,Value=$FUNCTION_NAME \
  --start-time "$(date -u -d '15 minutes ago' +%FT%TZ)" \
  --end-time "$(date -u +%FT%TZ)" \
  --period 60 \
  --statistics Sum \
  --query 'Datapoints|sort_by(@,&Timestamp)[-1]'
```

---

## 4. Custom Metrics & Dimensions

### 4.1 Publish multiple custom metrics
Syntax:
```bash
aws cloudwatch put-metric-data --namespace "<NAMESPACE>" --metric-data '[{...}]'
```
Example:
```bash
aws cloudwatch put-metric-data \
  --namespace "$NAMESPACE_CUSTOM" \
  --metric-data '[{"MetricName":"RequestCount","Value":1,"Unit":"Count"},{"MetricName":"LatencyMs","Value":132.4,"Unit":"Milliseconds"}]'
```

### 4.2 Publish with dimension
Syntax:
```bash
aws cloudwatch put-metric-data --namespace "<NS>" \
  --metric-data '[{"MetricName":"LatencyMs","Dimensions":[{"Name":"Service","Value":"<SERVICE_DIM>"}],"Value":<VAL>}]'
```
Example:
```bash
aws cloudwatch put-metric-data \
  --namespace "$NAMESPACE_CUSTOM" \
  --metric-data '[{"MetricName":"LatencyMs","Dimensions":[{"Name":"Service","Value":"Checkout"}],"Value":231.2}]'
```

### 4.3 High-resolution metric (periods < 60s)
Example (Storage resolution 1 second):
```bash
aws cloudwatch put-metric-data \
  --namespace "$NAMESPACE_CUSTOM" \
  --metric-data '[{"MetricName":"ProcessorUtil","Value":55.4,"Unit":"Percent","StorageResolution":1}]'
```

---

## 5. Metric Math & Anomaly Detection

### 5.1 Metric math (CPU to utilization ratio example)
Example:
```bash
aws cloudwatch get-metric-data \
  --metric-data-queries '[
    {
      "Id":"cpuavg",
      "MetricStat":{
        "Metric":{"Namespace":"AWS/EC2","MetricName":"CPUUtilization","Dimensions":[{"Name":"InstanceId","Value":"'"$INSTANCE_ID"'"}]},
        "Period":300,
        "Stat":"Average"
      },
      "ReturnData":false
    },
    {
      "Id":"scaled",
      "Expression":"cpuavg * 1",
      "Label":"CPU_Avg",
      "ReturnData":true
    }
  ]' \
  --start-time "$(date -u -d '1 hour ago' +%FT%TZ)" \
  --end-time "$(date -u +%FT%TZ)" \
  --query 'MetricDataResults[?Id==`scaled`].Values[0:5]'
```

### 5.2 Anomaly detection band (creates expression)
Example:
```bash
aws cloudwatch get-metric-data \
  --metric-data-queries '[
    {
      "Id":"m1",
      "MetricStat":{
        "Metric":{"Namespace":"AWS/EC2","MetricName":"CPUUtilization","Dimensions":[{"Name":"InstanceId","Value":"'"$INSTANCE_ID"'"}]},
        "Period":300,
        "Stat":"Average"
      },
      "ReturnData":true
    },
    {
      "Id":"ad1",
      "Expression":"ANOMALY_DETECTION_BAND(m1, 2)",
      "Label":"CPU (Band)",
      "ReturnData":true
    }
  ]' \
  --start-time "$(date -u -d '3 hours ago' +%FT%TZ)" \
  --end-time "$(date -u +%FT%TZ)"
```

---

## 6. Alarms (Standard, Composite, Anomaly)

### 6.1 Create CPU alarm
Syntax:
```bash
aws cloudwatch put-metric-alarm --alarm-name <ALARM_NAME> \
  --namespace AWS/EC2 --metric-name CPUUtilization \
  --dimensions Name=InstanceId,Value=<INSTANCE_ID> \
  --comparison-operator GreaterThanThreshold --threshold 80 \
  --evaluation-periods 2 --datapoints-to-alarm 2 \
  --period 60 --statistic Average \
  --treat-missing-data notBreaching
```
Example:
```bash
aws cloudwatch put-metric-alarm \
  --alarm-name $ALARM_CPU \
  --namespace AWS/EC2 \
  --metric-name CPUUtilization \
  --dimensions Name=InstanceId,Value=$INSTANCE_ID \
  --comparison-operator GreaterThanThreshold \
  --threshold 80 \
  --evaluation-periods 2 \
  --datapoints-to-alarm 2 \
  --period 60 \
  --statistic Average \
  --treat-missing-data notBreaching \
  --alarm-actions $SNS_TOPIC_ARN
```

### 6.2 Anomaly detection alarm
Example:
```bash
aws cloudwatch put-anomaly-detector \
  --namespace AWS/EC2 \
  --metric-name CPUUtilization \
  --statistic Average \
  --dimensions Name=InstanceId,Value=$INSTANCE_ID

aws cloudwatch put-metric-alarm \
  --alarm-name ec2-cpu-anomaly \
  --comparison-operator GreaterThanUpperThreshold \
  --evaluation-periods 3 \
  --datapoints-to-alarm 3 \
  --threshold-metric-id ad1 \
  --metrics '[
    {
      "Id":"m1",
      "MetricStat":{
        "Metric":{"Namespace":"AWS/EC2","MetricName":"CPUUtilization","Dimensions":[{"Name":"InstanceId","Value":"'"$INSTANCE_ID"'"}]},
        "Period":300,"Stat":"Average"
      }
    },
    {
      "Id":"ad1",
      "Expression":"ANOMALY_DETECTION_BAND(m1, 2)",
      "Label":"CPU_AD_BAND"
    }
  ]' \
  --alarm-actions $SNS_TOPIC_ARN
```

### 6.3 Composite alarm (depends on CPU + Lambda error alarm)
Example:
```bash
aws cloudwatch put-metric-alarm \
  --alarm-name lambda-errors \
  --namespace AWS/Lambda \
  --metric-name Errors \
  --dimensions Name=FunctionName,Value=$FUNCTION_NAME \
  --comparison-operator GreaterThanThreshold \
  --threshold 1 \
  --evaluation-periods 1 \
  --period 60 \
  --statistic Sum

aws cloudwatch put-composite-alarm \
  --alarm-name critical-composite \
  --alarm-rule "ALARM($ALARM_CPU) OR ALARM(lambda-errors)" \
  --alarm-actions $SNS_TOPIC_ARN
```

---

## 7. Alarm Management & Actions

### 7.1 Describe alarm
Syntax:
```bash
aws cloudwatch describe-alarms --alarm-names <ALARM_NAME>
```
Example:
```bash
aws cloudwatch describe-alarms --alarm-names $ALARM_CPU \
  --query 'MetricAlarms[0].{Name:AlarmName,State:StateValue,Updated:StateUpdatedTimestamp}'
```

### 7.2 Delete alarm
Syntax:
```bash
aws cloudwatch delete-alarms --alarm-names <ALARM_NAME>
```
Example:
```bash
aws cloudwatch delete-alarms --alarm-names ec2-cpu-anomaly
```

### 7.3 Set alarm state (test)
Syntax:
```bash
aws cloudwatch set-alarm-state --alarm-name <ALARM_NAME> --state-value ALARM --state-reason "Testing alarm"
```
Example:
```bash
aws cloudwatch set-alarm-state \
  --alarm-name $ALARM_CPU \
  --state-value ALARM \
  --state-reason "Manual test trigger"
```

### 7.4 Enable/disable actions
Example:
```bash
aws cloudwatch disable-alarm-actions --alarm-names $ALARM_CPU
aws cloudwatch enable-alarm-actions  --alarm-names $ALARM_CPU
```

### 7.5 List alarms by state
Example:
```bash
aws cloudwatch describe-alarms --state-value ALARM \
  --query 'MetricAlarms[].AlarmName'
```

---

## 8. Logs: Groups, Streams, Retention

### 8.1 List log groups
Syntax:
```bash
aws logs describe-log-groups --query 'logGroups[].logGroupName'
```
Example:
```bash
aws logs describe-log-groups \
  --query 'logGroups[?starts_with(logGroupName,`/app/`)].logGroupName'
```

### 8.2 Create log group
Syntax:
```bash
aws logs create-log-group --log-group-name <LOG_GROUP>
```
Example:
```bash
aws logs create-log-group --log-group-name $LOG_GROUP
```

### 8.3 Set retention
Syntax:
```bash
aws logs put-retention-policy --log-group-name <LOG_GROUP> --retention-in-days <DAYS>
```
Example:
```bash
aws logs put-retention-policy --log-group-name $LOG_GROUP --retention-in-days 14
```

### 8.4 Delete log group
Syntax:
```bash
aws logs delete-log-group --log-group-name <LOG_GROUP>
```
Example:
```bash
aws logs delete-log-group --log-group-name /app/old/service
```

### 8.5 Create log stream
Syntax:
```bash
aws logs create-log-stream --log-group-name <LOG_GROUP> --log-stream-name <STREAM>
```
Example:
```bash
aws logs create-log-stream \
  --log-group-name $LOG_GROUP \
  --log-stream-name $LOG_STREAM
```

---

## 9. Log Ingestion (Manual / Sequence Tokens)

### 9.1 Put single log event
Syntax:
```bash
aws logs put-log-events \
  --log-group-name <LOG_GROUP> \
  --log-stream-name <STREAM> \
  --log-events '[{"timestamp":<EPOCH_MS>,"message":"<MESSAGE>"}]'
```
Example:
```bash
aws logs put-log-events \
  --log-group-name $LOG_GROUP \
  --log-stream-name $LOG_STREAM \
  --log-events '[{"timestamp":'$(date +%s000)',"message":"Startup complete"}]'
```

### 9.2 Get upload sequence token
Syntax:
```bash
aws logs describe-log-streams \
  --log-group-name <LOG_GROUP> \
  --log-stream-name-prefix <STREAM> \
  --query 'logStreams[0].uploadSequenceToken' --output text
```
Example:
```bash
TOKEN=$(aws logs describe-log-streams \
  --log-group-name $LOG_GROUP \
  --log-stream-name-prefix $LOG_STREAM \
  --query 'logStreams[0].uploadSequenceToken' --output text)
echo $TOKEN
```

### 9.3 Put subsequent batch (with token)
Example:
```bash
aws logs put-log-events \
  --log-group-name $LOG_GROUP \
  --log-stream-name $LOG_STREAM \
  --log-events '[{"timestamp":'$(date +%s000)',"message":"Health=OK"}]' \
  --sequence-token "$TOKEN"
```

---

## 10. Tail & Filter Patterns

### 10.1 Tail logs (last 10m)
Syntax:
```bash
aws logs tail <LOG_GROUP> --since 10m --follow
```
Example:
```bash
aws logs tail $LOG_GROUP --since 10m --follow
```

### 10.2 Filter pattern (ERROR lines)
Syntax:
```bash
aws logs filter-log-events --log-group-name <LOG_GROUP> --filter-pattern "ERROR" --limit <N>
```
Example:
```bash
aws logs filter-log-events \
  --log-group-name $LOG_GROUP \
  --filter-pattern "ERROR" \
  --limit 20 \
  --query 'events[].{T:timestamp,M:message}' --output table
```

---

## 11. CloudWatch Logs Insights Queries

### 11.1 Start query (last 15m, top 20)
Syntax:
```bash
aws logs start-query \
  --log-group-name <LOG_GROUP> \
  --start-time <EPOCH_START> \
  --end-time <EPOCH_END> \
  --query-string '<INSIGHTS_QUERY>'
```
Example:
```bash
QUERY_ID=$(aws logs start-query \
  --log-group-name $LOG_GROUP \
  --start-time $(date -d '15 minutes ago' +%s) \
  --end-time $(date +%s) \
  --query-string 'fields @timestamp, @message | sort @timestamp desc | limit 20' \
  --query 'queryId' --output text)
echo $QUERY_ID
```

### 11.2 Get query results
Syntax:
```bash
aws logs get-query-results --query-id <QUERY_ID>
```
Example (poll):
```bash
aws logs get-query-results --query-id $QUERY_ID \
  --query 'results[0:5]'
```

### 11.3 Multi-group query
Example:
```bash
aws logs start-query \
  --log-group-names $LOG_GROUP /aws/lambda/$FUNCTION_NAME \
  --start-time $(date -d '30 minutes ago' +%s) \
  --end-time $(date +%s) \
  --query-string 'fields @logStream, @message | filter @message like /ERROR/ | limit 25'
```

---

## 12. Subscription Filters (Lambda / Firehose / Kinesis)

### 12.1 Create subscription (Lambda target)
Syntax:
```bash
aws logs put-subscription-filter \
  --log-group-name <LOG_GROUP> \
  --filter-name <FILTER_NAME> \
  --filter-pattern "<PATTERN>" \
  --destination-arn <LAMBDA_ARN>
```
Example:
```bash
aws logs put-subscription-filter \
  --log-group-name $LOG_GROUP \
  --filter-name error-forwarder \
  --filter-pattern "ERROR" \
  --destination-arn arn:aws:lambda:us-east-1:123456789012:function:log-processor
```

### 12.2 List subscription filters
Example:
```bash
aws logs describe-subscription-filters --log-group-name $LOG_GROUP \
  --query 'subscriptionFilters[].{Name:filterName,Pattern:filterPattern,Dest:destinationArn}'
```

### 12.3 Delete subscription filter
Example:
```bash
aws logs delete-subscription-filter \
  --log-group-name $LOG_GROUP \
  --filter-name error-forwarder
```

---

## 13. Export Log Data to S3

### 13.1 Create export task (time-bounded)
Syntax:
```bash
aws logs create-export-task \
  --task-name <NAME> \
  --log-group-name <LOG_GROUP> \
  --from <EPOCH_START_MS> --to <EPOCH_END_MS> \
  --destination <S3_BUCKET> \
  [--destination-prefix <PREFIX>]
```
Example (last hour):
```bash
aws logs create-export-task \
  --task-name export-last-hour \
  --log-group-name $LOG_GROUP \
  --from $(($(date +%s -d '1 hour ago')*1000)) \
  --to   $(($(date +%s)*1000)) \
  --destination $S3_EXPORT_BUCKET \
  --destination-prefix exports/$LOG_GROUP
```

### 13.2 Describe export tasks
Example:
```bash
aws logs describe-export-tasks --query 'exportTasks[0:3].[taskId,status.code]'
```

---

## 14. Dashboards

### 14.1 Put (create/update) dashboard
Syntax:
```bash
aws cloudwatch put-dashboard --dashboard-name <NAME> --dashboard-body '<JSON>'
```
Example:
```bash
aws cloudwatch put-dashboard \
  --dashboard-name WebAppOverview \
  --dashboard-body '{
    "widgets":[
      {
        "type":"metric",
        "x":0,"y":0,"width":12,"height":6,
        "properties":{
          "metrics":[["AWS/EC2","CPUUtilization","InstanceId","'"$INSTANCE_ID"'"]],
          "period":300,
          "stat":"Average",
          "title":"EC2 CPU"
        }
      }
    ]
  }'
```

### 14.2 Get dashboard
```bash
aws cloudwatch get-dashboard --dashboard-name WebAppOverview --query 'DashboardBody'
```

### 14.3 Delete dashboard
```bash
aws cloudwatch delete-dashboards --dashboard-names WebAppOverview
```

---

## 15. Metrics (Lambda & Cross-Namespace Tips)

### 15.1 Combined metric get (Lambda duration & errors)
Example:
```bash
aws cloudwatch get-metric-data \
  --metric-data-queries '[
    {"Id":"errors","MetricStat":{"Metric":{"Namespace":"AWS/Lambda","MetricName":"Errors","Dimensions":[{"Name":"FunctionName","Value":"'"$FUNCTION_NAME"'"}]},"Period":300,"Stat":"Sum"},"ReturnData":true},
    {"Id":"duration","MetricStat":{"Metric":{"Namespace":"AWS/Lambda","MetricName":"Duration","Dimensions":[{"Name":"FunctionName","Value":"'"$FUNCTION_NAME"'"}]},"Period":300,"Stat":"Average"},"ReturnData":true}
  ]' \
  --start-time "$(date -u -d '1 hour ago' +%FT%TZ)" \
  --end-time "$(date -u +%FT%TZ)" \
  --query 'MetricDataResults[].{Metric:Label,Values:Values[0:3]}'
```

### 15.2 Alarm on metric math (Error %)
Example:
```bash
aws cloudwatch put-metric-alarm \
  --alarm-name lambda-error-rate-high \
  --comparison-operator GreaterThanThreshold \
  --threshold 1 \
  --evaluation-periods 2 \
  --datapoints-to-alarm 2 \
  --metrics '[
     {"Id":"inv","MetricStat":{"Metric":{"Namespace":"AWS/Lambda","MetricName":"Invocations","Dimensions":[{"Name":"FunctionName","Value":"'"$FUNCTION_NAME"'"}]},"Period":60,"Stat":"Sum"}},
     {"Id":"err","MetricStat":{"Metric":{"Namespace":"AWS/Lambda","MetricName":"Errors","Dimensions":[{"Name":"FunctionName","Value":"'"$FUNCTION_NAME"'"}]},"Period":60,"Stat":"Sum"}},
     {"Id":"rate","Expression":"(err/inv)*100","Label":"ErrorPercent","ReturnData":true}
   ]' \
  --treat-missing-data notBreaching \
  --alarm-actions $SNS_TOPIC_ARN
```

---

## 16. Best Practices
- Use high-resolution metrics only when sub-minute alerts justify cost.  
- Treat missing data appropriately: `notBreaching` for sparse metrics (avoids flaps).  
- Apply anomaly detection for dynamic baselines (seasonality).  
- Use composite alarms to reduce noise (aggregate multiple underlying alarms).  
- Set retention policies on all log groups to control cost.  
- Use structured JSON logging for improved Logs Insights queries.  
- Avoid unbounded custom dimension cardinality (cost explosion).  
- Prefer `get-metric-data` over multiple `get-metric-statistics` calls (batch).  
- Restrict subscription filters to necessary patterns; max two per log group historically (service quotas vary).  
- Use export tasks or Kinesis subscriptions for long-term archival outside CloudWatch Logs.  
- Tag log groups (via console/API) for lifecycle & cost attribution.

---

## 17. Troubleshooting
| Issue | Symptom | Likely Cause | Resolution |
|-------|---------|--------------|-----------|
| Missing datapoints | Gaps in graphs | Low or no metric emission | Increase emission frequency; verify dimensions match alarm |
| Alarm stuck INSUFFICIENT_DATA | Never transitions | No datapoints yet | Use `treat-missing-data notBreaching` or ensure metric published |
| `InvalidParameterCombination` (put-metric-data) | Error on publish | Bad unit / invalid dimension pattern | Check units; dimension name length & allowed chars |
| Subscription filter error | No events forwarded | Missing Lambda invoke permission | Add lambda permission with correct source ARN |
| Sequence token invalid | Put log events fails | Out-of-order token | Fetch latest token then retry with incremental backoff |
| Export task stuck PENDING | No progress | Large volume or insufficient IAM perms | Ensure `logs:CreateExportTask` allowed; wait or reduce time window |
| High CloudWatch Logs cost | Elevated charges | No retention / verbose debug logs | Set retention; reduce debug verbosity; compress or sample |
| Insights query slow | Long runtime | Large unfiltered dataset | Narrow time/window; add filter early in query |
| Duplicate custom metrics | Overcounting | Same metric name w/ different dimension sets | Standardize dimension schema; remove redundant emissions |
| Alarm flapping | Frequent ALARM/OK transitions | Threshold too tight / noisy metric | Increase evaluation periods or use anomaly detection |

---

## 18. Automation Snippets

### 18.1 Create alarm only if absent
```bash
EXISTS=$(aws cloudwatch describe-alarms --alarm-names $ALARM_CPU \
  --query 'MetricAlarms | length(@)')
if [ "$EXISTS" -eq 0 ]; then
  echo "Creating alarm $ALARM_CPU"
  aws cloudwatch put-metric-alarm \
    --alarm-name $ALARM_CPU \
    --namespace AWS/EC2 \
    --metric-name CPUUtilization \
    --dimensions Name=InstanceId,Value=$INSTANCE_ID \
    --comparison-operator GreaterThanThreshold \
    --threshold 80 \
    --evaluation-periods 2 \
    --datapoints-to-alarm 2 \
    --period 60 \
    --statistic Average \
    --treat-missing-data notBreaching
fi
```

### 18.2 Batch get multiple metrics
```bash
aws cloudwatch get-metric-data \
  --metric-data-queries '[
    {"Id":"cpu","MetricStat":{"Metric":{"Namespace":"AWS/EC2","MetricName":"CPUUtilization","Dimensions":[{"Name":"InstanceId","Value":"'"$INSTANCE_ID"'"}]},"Period":300,"Stat":"Average"}},
    {"Id":"netin","MetricStat":{"Metric":{"Namespace":"AWS/EC2","MetricName":"NetworkIn","Dimensions":[{"Name":"InstanceId","Value":"'"$INSTANCE_ID"'"}]},"Period":300,"Stat":"Sum"}}
  ]' \
  --start-time "$(date -u -d '1 hour ago' +%FT%TZ)" \
  --end-time "$(date -u +%FT%TZ)"
```

### 18.3 Export logs daily (cronable)
```bash
FROM=$(($(date -d '1 day ago 00:00:00' +%s)*1000))
TO=$(($(date -d '1 day ago 23:59:59' +%s)*1000))
aws logs create-export-task \
  --task-name daily-export-$(date -d '1 day ago' +%Y%m%d) \
  --log-group-name $LOG_GROUP \
  --from $FROM \
  --to $TO \
  --destination $S3_EXPORT_BUCKET \
  --destination-prefix daily/$LOG_GROUP/
```

---

## 19. Cleanup Sequences

### 19.1 Delete alarms by prefix
```bash
aws cloudwatch describe-alarms --query 'MetricAlarms[].AlarmName' --output text | \
  tr '\t' '\n' | grep '^ec2-' | while read A; do
    aws cloudwatch delete-alarms --alarm-names "$A"
  done
```

### 19.2 Remove anomaly detector
```bash
aws cloudwatch delete-anomaly-detector \
  --namespace AWS/EC2 \
  --metric-name CPUUtilization \
  --statistic Average \
  --dimensions Name=InstanceId,Value=$INSTANCE_ID
```

### 19.3 Delete log group & its subscriptions
```bash
aws logs describe-subscription-filters --log-group-name $LOG_GROUP \
  --query 'subscriptionFilters[].filterName' --output text | tr '\t' '\n' | while read F; do
    aws logs delete-subscription-filter --log-group-name $LOG_GROUP --filter-name "$F"
  done
aws logs delete-log-group --log-group-name $LOG_GROUP
```

### 19.4 Remove dashboard(s)
```bash
aws cloudwatch delete-dashboards --dashboard-names WebAppOverview
```

---

## 20. Notes
- High-resolution metrics (1-second) increase cost; use sparingly for critical latency/ops.  
- Logs Insights pricing is based on scanned data; narrow queries.  
- Metrics older than 15 months are dropped (standard retention).  
- Consider exporting logs to S3 + Athena for long-term analytics.  
- A single custom metric with many unique dimension value combinations raises costs—limit cardinality.  
- Composite alarms help consolidate notifications (one page for multiple root causes).  
- Use standardized dimension set: e.g., Service, Environment, InstanceId.  

---
